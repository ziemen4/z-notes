{"componentChunkName":"component---src-templates-draft-js","path":"/draft/56561063-7248-4eef-b8e0-2f2e5f6d68b7/","result":{"data":{"markdownRemark":{"html":"<h3>Introduction</h3>\n<p>Recent advances in both <a href=\"https://0xparc.org/blog/programmable-cryptography-1\">programmable cryptography</a> and <a href=\"https://ourworldindata.org/grapher/test-scores-ai-capabilities-relative-human-performance\">AI</a> may appear unrelated at first glance, but they have a clear overlap.</p>\n<p>Programmable cryptography offers two key capabilities:</p>\n<ol>\n<li><strong>Private computation</strong>: Allows an external party to execute a computation on your data without learning anything about the data itself.</li>\n<li><strong>Verifiable computation</strong>: Allows one to efficiently check that the party carried out exactly a predefined computation.</li>\n</ol>\n<p>![[Pasted image 20250426111800.png]]\n<em>A simplified tree of cryptographic primitives. Source: <a href=\"https://0xparc.org/blog/programmable-cryptography-1\">programmable-cryptography</a></em></p>\n<p>AI systems excel at narrowly scoped tasks and are gradually tackling <a href=\"https://contextual.ai/blog/plotting-progress-in-ai/\">broader challenges</a> such as prediction and reasoning. These systems do not follow a single, predetermined algorithm but learn from data, producing behavior that may be far from trivial to interpret. This <em>emergence</em> of behavior is of increasing interest as AI takes over more and more important tasks, its <a href=\"https://www.ibm.com/think/topics/explainable-ai\">explainability</a> is an active area of research.</p>\n<p>As AI systems advance and surpass humans across an increasing range of tasks, a critical question arises:</p>\n<ul>\n<li><strong>How can we verify that an AI system is behaving as expected?</strong></li>\n</ul>\n<p>There are two relevant parts to this question:</p>\n<ol>\n<li>Understanding how an AI system is behaving in practice</li>\n<li>Properly defining what its expected behavior should be</li>\n</ol>\n<p>In other words, we are looking to see how <em>aligned</em> the system is (how it should behave) with respect to some objective (its expected behavior).</p>\n<h3>Programmable Cryptography</h3>\n<p>Here, I adopt the definition of the <a href=\"https://0xparc.org/blog/programmable-cryptography-1\">article</a> linked before.</p>\n<blockquote>\n<p>\"*We use the term “programmable cryptography” to refer to a second generation of cryptographic primitives that are becoming practical today. The defining feature of these primitives is that they are far more flexible than first-generation cryptography: they allow us to perform general-purpose computation inside or on top of cryptographic protocols.\"</p>\n</blockquote>\n<p>In this discussion, our focus is on Succint Non-Interactive ARguments of Knowledge (SNARKs) and their role in <strong>verifying</strong> the actions of an AI system. Although SNARKs also support privacy (as zkSNARKs), the primary interest regarding AI alignment has to do with its verifiability property. This is because <em>alignment</em> (something we dicuss later) depends on our ability to confirm that an AI system performed the computation we expected.</p>\n<p>This area of research, widely known as <a href=\"https://medium.com/@vid.kersic/demystifying-zkml-0f3dff7194b9\">zkML</a> is advancing rapidly by merging cryptography and AI.</p>\n<h3>AI Alignment</h3>\n<p>AI alignment is <a href=\"https://www.amazon.com/Artificial-Intelligence-A-Modern-Approach/dp/0134610997\">defined</a> with respect to an <em>objective</em>. If the AI system advances towards the objective we say that it is <strong>aligned</strong>, while if it deviates we say it is pursuing unintended objectives.</p>\n<p>The nature of the objective depends on the system. A system such as <a href=\"https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/\">AlphaZero</a> simply must win in the game of Go, while others like <a href=\"https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\">AlphaProof</a> are trained on formal problems and their objective is to correctly derive a proof</p>\n<p>![[Pasted image 20250426105757.png]]\n<em>A representation of the training process of AlphaProof: (Source: <a href=\"https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\">blog</a>)</em></p>\n<p>When objectives are well-defined, it is simpler to try to understand when an AI system is not aligned (and notably <a href=\"https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\">Reinforcement Learning (RL)</a> techniques are well-suited to this kind of problems, because of their verifiability).</p>\n<p>On the other hand, modern large-scale models (such as today's LLMs) do not have such neatly defined objectives. Their <em>alignment</em> emerges from the training process, (ie learning to be a helpful assistant), but this doesn't mean that their behaviour will be predictable for some <a href=\"https://www.holisticai.com/red-teaming/chatgpt-4-5-jailbreaking-red-teaming\">out of distribution input</a>. As AI systems assume roles once performed by humans, the incentive to verify their behavior grows ever stronger.</p>\n<h3>Using SNARKs for alignment verification</h3>\n<p>Recent <a href=\"https://arxiv.org/pdf/2402.02675\">research</a> (and the main catalyst for this post) has been looking for answers on the verifiability of AI systems. Specifically, on how to use SNARKs to verify the behavior of machine learning models, even when those models are closed-source.</p>\n<p>In this paper, the authors propose the use of <em>benchmarks</em> and <em>proofs of inference</em> (proving a certain model performed the computation).</p>\n<blockquote>\n<p><em>\"The goal of this work is to remove the need for the public or an end user to trust the model provider. The zkSNARKs enable verification that computational work with a model with weights <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>H</mi><mo stretchy=\"false\">(</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">H(W)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mclose\">)</span></span></span></span></span> occurred, that it produced a given benchmark, and that it was used for a specific inference that is challenged\"</em></p>\n</blockquote>\n<p>Specifically, when a user questions an output, the provider must produce a succinct proof showing that the model with the same weights that generated the published benchmark indeed produced the claimed output for the challenged input.</p>\n<p>To get into more detail, we are given a <em>benchmark</em> which aggregates <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x, y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span></span> pairs that are being ran on the model (ie proving accuracy for a certain task is >70%) giving us an indication of how the model \"behaves\" (if we can aggregate information in a certain way, we may indirectly understand how a model behaves in a general sense) though always subject to the dataset used to create the input-output pairs, which is of course a critical aspect.</p>\n<p>![[Screenshot 2025-05-05 at 21.10.46.png]]\n*A system diagram of verifiable ML evaluation: (Source: <a href=\"https://arxiv.org/pdf/2402.02675\">paper</a>)</p>\n<p>To challenge any response, whoever receives an output <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>y</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">y'</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9463em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7519em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span></span> can ask for a <em>proof of inference</em>, the provider must then create said proof, which must show that the model used weights <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span> (the same one that produced the benchmark) to produce said output and that it was based on the input <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>x</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">x'</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7519em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7519em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span></span> sent by the user.</p>\n<p>Leveraging ZKPs, we have managed a way to <strong>verify</strong> that a closed-source AI system being used is the same as a system we know behaves in a certain way!</p>\n<h3>A quick detour: Isaac Asimov's three laws of robotics</h3>\n<p>A long time ago, in 1942, Isaac Asimov published \"Runaround\" a short story around the infamous <strong>three laws of robotics</strong> :</p>\n<blockquote>\n<ol>\n<li>A robot may not injure a human being or, through inaction, allow a human being to come to harm.</li>\n<li>A robot must obey orders given it by human beings except where such orders would conflict with the First Law.</li>\n<li>A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.</li>\n</ol>\n</blockquote>\n<p>My unfounded belief is that this implied some sort of hardwired constraints in the robot's programming, but as is well known, AI is mostly based on NNs which resemble something more akin to human cognition than traditional programming</p>\n<p>Were we to deploy robots governed by these laws today, how could we verify that a robot is actually complying with said rules? Though we still don't have independent intelligent robots running around in our daily lives (but steady <a href=\"https://youtu.be/I44_zbEwz_w?si=dSXjHNys9-CBQoAZ\">progress is being done</a>), thinking about AI alignment is something we must do to ensure that <em>any</em> artificial intelligence model that is used behaves as we expect it to.</p>\n<h3>Rambling: AI alignment in the age of Robots and AGI</h3>\n<p>As a speculative exercise, imagine combining Asimov’s laws with SNARK-based verification. Theoretically (and in a very broad and ambiguous sense), how could we achieve something like that?</p>\n<p>Well, based on what we discussed, perhaps we could create a dynamic <em>benchmark</em> managed by society which probes the closed-source systems to indirectly understand what their expected behavior is. These benchmarks must be very elaborate, perhaps aggregating complex test cases into an alignment score (maybe the score could be also automated by an AI system, though we would need that to be compliant as well!)</p>\n<p>Maybe a safer approach would be having open-source models, which can potentially simplify the process of understanding behaviour by using tools in <a href=\"https://www.ibm.com/think/topics/explainable-ai\">XAI</a>. Though that would seem to hinder competition between private companies, <a href=\"https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/\">or perhaps not</a>. I guess we will learn more about it as we see how open and close source models grow in the coming years.</p>\n<p>In an ideal future where proof generation becomes almost instantaneous, every robot action could be accompanied by a ZKP. Verifying each proof would ensure compliance with alignment constraints, reducing the risk of unsafe or unintended behavior. Maybe this process can be the core process that any robot should have, and all security aspects should be linked to it.</p>\n<p>If this actually works somewhat well, issues such as model impersonation or model hacking (which could lead to inappropriate or unsafe behavior), would be hard! Though there is always an attack surface for the core process itself.</p>\n<p>Despite the highly speculative nature (and perhaps complete nonsense) of this last section, it illustrates how verifiable computation will play a pivotal role in the responsible deployment of AI systems in the future.</p>","frontmatter":{"title":"AI alignment through programmable cryptography","date":"May 05, 2025"}}},"pageContext":{"id":"bf025209-a1a0-564f-8a5a-1bc2bc1a5a33"}},"staticQueryHashes":["2841359383"],"slicesMap":{}}